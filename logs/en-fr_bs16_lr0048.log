INFO: COMMAND: train.py --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/output/en-fr_bs16_lr0048/checkpoints/ --log-file logs/en-fr_bs16_lr0048.log --batch-size 16 --lr 0.0048
INFO: Arguments: {'cuda': False, 'data': 'data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 16, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0048, 'patience': 3, 'log_file': 'logs/en-fr_bs16_lr0048.log', 'save_dir': 'assignments/03/output/en-fr_bs16_lr0048/checkpoints/', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 4.648 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.83 | clip 0.9952
INFO: Epoch 000: valid_loss 4.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 98.3
INFO: Epoch 001: loss 3.807 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 6.78 | clip 0.9824
INFO: Epoch 001: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.5
INFO: Epoch 002: loss 3.498 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 6.822 | clip 0.9888
INFO: Epoch 002: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.1
INFO: Epoch 003: loss 3.246 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 7.091 | clip 0.9808
INFO: Epoch 003: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.8
INFO: Epoch 004: loss 3.037 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 7.338 | clip 0.9824
INFO: Epoch 004: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.9
INFO: Epoch 005: loss 2.851 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 7.731 | clip 0.9888
INFO: Epoch 005: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.6
INFO: Epoch 006: loss 2.687 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 7.992 | clip 0.992
INFO: Epoch 006: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.4
INFO: Epoch 007: loss 2.537 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.177 | clip 0.9904
INFO: Epoch 007: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5
INFO: Epoch 008: loss 2.41 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.452 | clip 0.9952
INFO: Epoch 008: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21
INFO: Epoch 009: loss 2.301 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.553 | clip 0.9936
INFO: Epoch 009: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8
INFO: Epoch 010: loss 2.199 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.646 | clip 0.9936
INFO: Epoch 010: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6
INFO: Epoch 011: loss 2.116 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.672 | clip 0.9952
INFO: Epoch 011: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.8
INFO: Epoch 012: loss 2.04 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.777 | clip 1
INFO: Epoch 012: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2
INFO: Epoch 013: loss 1.975 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.789 | clip 0.9968
INFO: Epoch 013: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4
INFO: Epoch 014: loss 1.926 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.846 | clip 0.9968
INFO: Epoch 014: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2
INFO: Epoch 015: loss 1.871 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.871 | clip 0.9968
INFO: Epoch 015: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1
INFO: Epoch 016: loss 1.82 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.954 | clip 0.9984
INFO: Epoch 016: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9
INFO: Epoch 017: loss 1.777 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.944 | clip 0.9952
INFO: Epoch 017: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1
INFO: Epoch 018: loss 1.742 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.883 | clip 0.9952
INFO: Epoch 018: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15
INFO: Epoch 019: loss 1.703 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 9.014 | clip 0.9952
INFO: Epoch 019: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7
INFO: Epoch 020: loss 1.675 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.904 | clip 0.9952
INFO: Epoch 020: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3
INFO: Epoch 021: loss 1.637 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.885 | clip 0.9904
INFO: Epoch 021: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1
INFO: Epoch 022: loss 1.618 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.937 | clip 0.9968
INFO: Epoch 022: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6
INFO: Epoch 023: loss 1.593 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.999 | clip 0.9904
INFO: Epoch 023: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14
INFO: Epoch 024: loss 1.57 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 9.008 | clip 0.9904
INFO: Epoch 024: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1
INFO: Epoch 025: loss 1.552 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.985 | clip 0.9904
INFO: Epoch 025: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3
INFO: Epoch 026: loss 1.529 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 9.033 | clip 0.9936
INFO: Epoch 026: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5
INFO: Epoch 027: loss 1.518 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.905 | clip 0.992
INFO: Epoch 027: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2
INFO: Epoch 028: loss 1.49 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.958 | clip 0.9936
INFO: Epoch 028: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8
INFO: Epoch 029: loss 1.473 | lr 0.0048 | num_tokens 9.1 | batch_size 16 | grad_norm 8.906 | clip 0.992
INFO: Epoch 029: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6
INFO: No validation set improvements observed for 3 epochs. Early stop!
