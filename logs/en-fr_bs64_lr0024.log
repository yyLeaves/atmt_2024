INFO: COMMAND: train.py --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/output/en-fr_bs64_lr0024/checkpoints/ --log-file logs/en-fr_bs64_lr0024.log --batch-size 64 --lr 0.0024
INFO: Arguments: {'cuda': False, 'data': 'data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 64, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0024, 'patience': 3, 'log_file': 'logs/en-fr_bs64_lr0024.log', 'save_dir': 'assignments/03/output/en-fr_bs64_lr0024/checkpoints/', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 5.193 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.687 | clip 0.9745
INFO: Epoch 000: valid_loss 5.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 185
INFO: Epoch 001: loss 4.482 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.28 | clip 0.9045
INFO: Epoch 001: valid_loss 4.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 113
INFO: Epoch 002: loss 4.085 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.569 | clip 0.7771
INFO: Epoch 002: valid_loss 4.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.5
INFO: Epoch 003: loss 3.811 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.144 | clip 0.7452
INFO: Epoch 003: valid_loss 4.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.9
INFO: Epoch 004: loss 3.599 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.952 | clip 0.7389
INFO: Epoch 004: valid_loss 3.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.9
INFO: Epoch 005: loss 3.412 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.11 | clip 0.7325
INFO: Epoch 005: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.4
INFO: Epoch 006: loss 3.271 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.029 | clip 0.7134
INFO: Epoch 006: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.4
INFO: Epoch 007: loss 3.138 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.119 | clip 0.6943
INFO: Epoch 007: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.4
INFO: Epoch 008: loss 3.01 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.167 | clip 0.7389
INFO: Epoch 008: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.6
INFO: Epoch 009: loss 2.892 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.276 | clip 0.758
INFO: Epoch 009: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.9
INFO: Epoch 010: loss 2.776 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.446 | clip 0.7707
INFO: Epoch 010: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24
INFO: Epoch 011: loss 2.66 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.54 | clip 0.7898
INFO: Epoch 011: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.3
INFO: Epoch 012: loss 2.554 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.642 | clip 0.8089
INFO: Epoch 012: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9
INFO: Epoch 013: loss 2.448 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.846 | clip 0.828
INFO: Epoch 013: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5
INFO: Epoch 014: loss 2.36 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.907 | clip 0.8535
INFO: Epoch 014: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6
INFO: Epoch 015: loss 2.28 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.97 | clip 0.8535
INFO: Epoch 015: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16
INFO: Epoch 016: loss 2.198 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.064 | clip 0.8471
INFO: Epoch 016: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2
INFO: Epoch 017: loss 2.136 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.195 | clip 0.879
INFO: Epoch 017: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2
INFO: Epoch 018: loss 2.069 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.166 | clip 0.8535
INFO: Epoch 018: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6
INFO: Epoch 019: loss 2.008 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.224 | clip 0.8854
INFO: Epoch 019: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5
INFO: Epoch 020: loss 1.95 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.367 | clip 0.8981
INFO: Epoch 020: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8
INFO: Epoch 021: loss 1.896 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.333 | clip 0.8726
INFO: Epoch 021: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4
INFO: Epoch 022: loss 1.846 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.338 | clip 0.8854
INFO: Epoch 022: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8
INFO: Epoch 023: loss 1.797 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.322 | clip 0.8854
INFO: Epoch 023: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4
INFO: Epoch 024: loss 1.765 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.495 | clip 0.9045
INFO: Epoch 024: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5
INFO: Epoch 025: loss 1.714 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.446 | clip 0.8662
INFO: Epoch 025: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7
INFO: Epoch 026: loss 1.68 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.483 | clip 0.8854
INFO: Epoch 026: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2
INFO: Epoch 027: loss 1.652 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.544 | clip 0.8854
INFO: Epoch 027: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12
INFO: Epoch 028: loss 1.618 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.486 | clip 0.879
INFO: Epoch 028: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12
INFO: Epoch 029: loss 1.59 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.55 | clip 0.8917
INFO: Epoch 029: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12
INFO: Epoch 030: loss 1.554 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.5 | clip 0.8854
INFO: Epoch 030: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5
INFO: Epoch 031: loss 1.533 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.728 | clip 0.8854
INFO: Epoch 031: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5
INFO: Epoch 032: loss 1.501 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.54 | clip 0.8726
INFO: Epoch 032: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1
INFO: Epoch 033: loss 1.473 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.547 | clip 0.8854
INFO: Epoch 033: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3
INFO: Epoch 034: loss 1.451 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.722 | clip 0.8599
INFO: Epoch 034: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3
INFO: Epoch 035: loss 1.429 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.573 | clip 0.8408
INFO: Epoch 035: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4
INFO: Epoch 036: loss 1.403 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.554 | clip 0.8726
INFO: Epoch 036: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7
INFO: Epoch 037: loss 1.383 | lr 0.0024 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.609 | clip 0.8599
INFO: Epoch 037: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4
INFO: No validation set improvements observed for 3 epochs. Early stop!
